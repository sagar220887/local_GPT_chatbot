{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install --upgrade --quiet  lark chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.documents import Document\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of document - 253\n",
      "Date: 6/20/2019\n",
      "Open: 190.949997\n",
      "High: 191.160004\n",
      "Low: 187.639999\n",
      "Close: 189.529999\n",
      "Adj Close: 189.529999\n",
      "Volume: 14635700\n"
     ]
    }
   ],
   "source": [
    "# Loaading data\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "loader = CSVLoader('data/FB_data.csv')\n",
    "documents = loader.load()\n",
    "print(f'Length of document - {len(documents)}') # Number of documents = No of rows in the csv file\n",
    "print(documents[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documents type -  <class 'list'>\n",
      "documents length -  253\n",
      "page_content='Date: 6/20/2019\\nOpen: 190.949997\\nHigh: 191.160004\\nLow: 187.639999\\nClose: 189.529999\\nAdj Close: 189.529999\\nVolume: 14635700' metadata={'source': 'data/FB_data.csv', 'row': 0}\n"
     ]
    }
   ],
   "source": [
    "## splitting the document into chunks\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "recursive_char_text_splitter=RecursiveCharacterTextSplitter(\n",
    "                                                chunk_size=500,\n",
    "                                                chunk_overlap=50)\n",
    "split_documents=recursive_char_text_splitter.split_documents(documents)\n",
    "print('documents type - ', type(split_documents))\n",
    "print('documents length - ', len(split_documents))\n",
    "print(split_documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\sagar\\work\\AI_ML\\gitrepo\\GENAI\\YOUTUBES\\Talk_with_your_data\\local_GPT_chatbot\\localgpt_venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "## Create embeddings for each document\n",
    "\n",
    "embeddings=HuggingFaceEmbeddings(\n",
    "            model_name='sentence-transformers/all-MiniLM-L6-v2', \n",
    "            model_kwargs={'device':'cpu'}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(split_documents, embeddings) # <langchain_community.vectorstores.chroma.Chroma object at 0x0000022559FBBCD0>\n",
    "\n",
    "\n",
    "# from langchain.vectorstores import FAISS\n",
    "# vectorstore =FAISS.from_documents(split_documents, embeddings) # <langchain.vectorstores.faiss.FAISS object at 0x0000022559FBC250>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"source\",\n",
    "        description=\"the name of the file containing the data\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"row\",\n",
    "        description=\"The row number the data corresponding to\",\n",
    "        type=\"integer\",\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\sagar\\work\\AI_ML\\gitrepo\\GENAI\\YOUTUBES\\Talk_with_your_data\\local_GPT_chatbot\\localgpt_venv\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'InferenceApi' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mHuggingFaceHub\u001b[0m\n",
      "Params: {'repo_id': 'mistralai/Mistral-7B-Instruct-v0.2', 'task': None, 'model_kwargs': {'max_new_tokens': 512, 'temperature': 0.01}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from langchain import HuggingFaceHub\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "HF_API_KEY = os.getenv('HUGGINGFACE_API_KEY')\n",
    "MODEL_REPO_ID = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "llm = HuggingFaceHub(\n",
    "            repo_id=MODEL_REPO_ID, \n",
    "            huggingfacehub_api_token=HF_API_KEY,\n",
    "            model_kwargs={'max_new_tokens':512,\n",
    "                    'temperature':0.01\n",
    "            }\n",
    "        )\n",
    "\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Self query retriever with Vector Store type <class 'langchain_community.vectorstores.chroma.Chroma'> not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m document_content_description \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m retriever \u001b[38;5;241m=\u001b[39m \u001b[43mSelfQueryRetriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_llm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvectorstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocument_content_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata_field_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\sagar\\work\\AI_ML\\gitrepo\\GENAI\\YOUTUBES\\Talk_with_your_data\\local_GPT_chatbot\\localgpt_venv\\Lib\\site-packages\\langchain\\retrievers\\self_query\\base.py:138\u001b[0m, in \u001b[0;36mSelfQueryRetriever.from_llm\u001b[1;34m(cls, llm, vectorstore, document_contents, metadata_field_info, structured_query_translator, chain_kwargs, enable_limit, use_original_query, **kwargs)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_llm\u001b[39m(\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    136\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSelfQueryRetriever\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m structured_query_translator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 138\u001b[0m         structured_query_translator \u001b[38;5;241m=\u001b[39m \u001b[43m_get_builtin_translator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvectorstore\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    139\u001b[0m     chain_kwargs \u001b[38;5;241m=\u001b[39m chain_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallowed_comparators\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m chain_kwargs:\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\sagar\\work\\AI_ML\\gitrepo\\GENAI\\YOUTUBES\\Talk_with_your_data\\local_GPT_chatbot\\localgpt_venv\\Lib\\site-packages\\langchain\\retrievers\\self_query\\base.py:43\u001b[0m, in \u001b[0;36m_get_builtin_translator\u001b[1;34m(vectorstore)\u001b[0m\n\u001b[0;32m     35\u001b[0m BUILTIN_TRANSLATORS: Dict[Type[VectorStore], Type[Visitor]] \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     36\u001b[0m     Pinecone: PineconeTranslator,\n\u001b[0;32m     37\u001b[0m     Chroma: ChromaTranslator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     40\u001b[0m     MyScale: MyScaleTranslator,\n\u001b[0;32m     41\u001b[0m }\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vectorstore_cls \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m BUILTIN_TRANSLATORS:\n\u001b[1;32m---> 43\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     44\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSelf query retriever with Vector Store type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvectorstore_cls\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     45\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     46\u001b[0m     )\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(vectorstore, Qdrant):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m QdrantTranslator(metadata_key\u001b[38;5;241m=\u001b[39mvectorstore\u001b[38;5;241m.\u001b[39mmetadata_payload_key)\n",
      "\u001b[1;31mValueError\u001b[0m: Self query retriever with Vector Store type <class 'langchain_community.vectorstores.chroma.Chroma'> not supported."
     ]
    }
   ],
   "source": [
    "document_content_description = \"facebook dataset\"\n",
    "\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm,\n",
    "    vectorstore,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
